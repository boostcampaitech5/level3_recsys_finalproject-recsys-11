{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/howcanai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Required Python 3.10 version \n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AdamWeightDecay, AdamW, get_linear_schedule_with_warmup\n",
    "from adamp import AdamP\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, TextClassificationPipeline \n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentCLSModel(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(IntentCLSModel, self).__init__()\n",
    "        self.save_hyperparameters() # self.hparams에 config 저장됨.\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "        \n",
    "        self.config = config\n",
    "        self.bert = AutoModel.from_pretrained(self.config.model)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, self.config.n_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "    def forward(self, *args):\n",
    "        output = self.bert(*args)\n",
    "        pred = self.fc(output.pooler_output)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        assert self.config.optimizer in ['AdamW', 'AdamP'], 'Only AdamW, AdamP'\n",
    "        \n",
    "        if self.config.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(self.parameters(), lr=self.config.lr, eps=self.config.adam_eps)\n",
    "        elif self.config.optimizer == 'AdamP':\n",
    "            optimizer = AdamP(self.parameters(), lr=self.config.lr, eps=self.config.adam_eps)\n",
    "            \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "            \n",
    "        return {'optimizer': optimizer,\n",
    "                'scheduler': scheduler\n",
    "                }\n",
    "          \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        y = batch['label']\n",
    "        y_hat = self.forward(input_ids, attention_mask)\n",
    "        \n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        y = batch['label']\n",
    "        y_hat = self.forward(input_ids, attention_mask)\n",
    "        \n",
    "        loss = self.criterion(y_hat, y)\n",
    "        output = {'loss': loss, 'batch_labels': y, 'batch_preds': y_hat}\n",
    "        self.validation_step_outputs.append(output)\n",
    "        return output\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_labels = torch.cat([x['batch_labels'] for x in self.validation_step_outputs])\n",
    "        epoch_preds = torch.cat([x['batch_preds'] for x in self.validation_step_outputs])\n",
    "        epoch_loss = self.criterion(epoch_preds, epoch_labels)\n",
    "        \n",
    "        corrects = (epoch_preds.argmax(dim=1) == epoch_labels).sum().item() \n",
    "        epoch_acc = corrects / len(epoch_labels)\n",
    "        self.log('val_loss', epoch_loss, on_epoch=True, logger=True)\n",
    "        self.log('val_acc', epoch_acc, on_epoch=True, logger=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "        return {'val_loss': epoch_loss, 'val_acc': epoch_acc}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        y = batch['label']\n",
    "        y_hat = self.forward(input_ids, attention_mask)\n",
    "        \n",
    "        loss = self.criterion(y_hat, y)\n",
    "        output = {'loss': loss, 'batch_labels': y, 'batch_preds': y_hat}\n",
    "        self.test_step_outputs.append(output)\n",
    "        return output\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        epoch_labels = torch.cat([x['batch_labels'].detach().cpu() for x in self.test_step_outputs])\n",
    "        epoch_preds = torch.cat([x['batch_preds'].detach().cpu() for x in self.test_step_outputs])\n",
    "        # epoch_loss = self.criterion(epoch_preds, epoch_labels)\n",
    "        \n",
    "        acc = accuracy_score(y_true=epoch_labels, y_pred=np.argmax(epoch_preds, axis=1))\n",
    "        # average micro macro weighted\n",
    "        metrics = [metric(y_true=epoch_labels, y_pred=np.argmax(epoch_preds, axis=1), average='macro' )\n",
    "                   for metric in (precision_score, recall_score, f1_score)]\n",
    "        \n",
    "        # self.log('test_loss', epoch_loss, on_epoch=True, logger=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, logger=True)\n",
    "        self.log('test_precision', metrics[0], on_epoch=True, logger=True)\n",
    "        self.log('test_recall', metrics[1], on_epoch=True, logger=True)\n",
    "        self.log('test_f1', metrics[2], on_epoch=True, logger=True)\n",
    "        self.test_step_outputs.clear()\n",
    "        return {'test_acc': acc, \n",
    "                'test_precision': metrics[0], 'test_recall': metrics[1], \n",
    "                'test_f1': metrics[2]\n",
    "                }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- roberta-epoch=4-val_loss=0.214.ckpt => roberta-base\n",
    "- roberta-epoch=2-val_loss=0.209.ckpt => roberta-base\n",
    "- roberta-epoch=1-val_loss=0.199.ckpt => roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(query:str, model:str, ckpt_path:str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    best_model = IntentCLSModel.load_from_checkpoint(checkpoint_path=ckpt_path).to('cpu')\n",
    "    \n",
    "    category_list = ['거래 의도 (Transactional Intent) - 여행 예약 (Travel Reservations)',\n",
    "                     '거래 의도 (Transactional Intent) - 예약 및 예매 (Reservations and Bookings)',\n",
    "                     '거래 의도 (Transactional Intent) - 음식 주문 및 배달 (Food Ordering and Delivery)',\n",
    "                     '거래 의도 (Transactional Intent) - 이벤트 티켓 예매 (Event Ticket Booking)',\n",
    "                     '거래 의도 (Transactional Intent) - 제품 구매 (Product Purchase)',\n",
    "                     '네비게이셔널 의도 (Navigational Intent) - 대중교통 및 지도 (Public Transportation and Maps)',\n",
    "                     '네비게이셔널 의도 (Navigational Intent) - 여행 및 관광 (Travel and Tourism)',\n",
    "                     '네비게이셔널 의도 (Navigational Intent) - 웹사이트/앱 검색 (Website/App Search)',\n",
    "                     '네비게이셔널 의도 (Navigational Intent) - 호텔 및 숙박 (Hotels and Accommodation)',\n",
    "                     '상업적 정보 조사 의도 (Commercial Intent) - 가전제품 (Electronics)',\n",
    "                     '상업적 정보 조사 의도 (Commercial Intent) - 식품 및 요리 레시피 (Food and Recipe)',\n",
    "                     '상업적 정보 조사 의도 (Commercial Intent) - 제품 가격 비교 (Product Price Comparison)',\n",
    "                     '상업적 정보 조사 의도 (Commercial Intent) - 제품 리뷰 (Product Reviews)',\n",
    "                     '상업적 정보 조사 의도 (Commercial Intent) - 패션 및 뷰티 (Fashion and Beauty)',\n",
    "                     '정보 제공 의도 (Informational Intent) - 건강 및 의학 (Health and Medicine)',\n",
    "                     '정보 제공 의도 (Informational Intent) - 과학 및 기술 (Science and Technology)',\n",
    "                     '정보 제공 의도 (Informational Intent) - 역사 (History)',\n",
    "                     '정보 제공 의도 (Informational Intent) - 인물 정보 (Biographies)',\n",
    "                     '정보 제공 의도 (Informational Intent) - 일반 지식 (General Knowledge)',\n",
    "                     '정보 제공 의도 (Informational Intent) - 정치, 사회, 경제 (Politics, Society, Economy)']\n",
    "    \n",
    "    best_model.eval()\n",
    "    best_model.freeze()\n",
    "    \n",
    "    tokens = tokenizer.encode_plus(\n",
    "            query,\n",
    "            return_tensors='pt',\n",
    "            max_length=32,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            # pad_to_max_length=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "    \n",
    "    pred = best_model(tokens['input_ids'], tokens['attention_mask'])\n",
    "    output_idx = pred.argmax().item()\n",
    "    cat = category_list[output_idx]\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'거래 의도 (Transactional Intent) - 제품 구매 (Product Purchase)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qurey = '아이폰 14 가격'\n",
    "\n",
    "inference(query=qurey, model='klue/roberta-base', ckpt_path=os.path.join('./ckpt/', 'roberta-epoch=5-val_loss=0.207.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "howcanai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
